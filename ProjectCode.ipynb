{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81f737d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T06:25:48.493482Z",
     "start_time": "2022-12-04T06:25:44.598891Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "039f6571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T19:06:28.120077Z",
     "start_time": "2022-12-07T19:06:28.115758Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82dfa427",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-04T06:25:49.703775Z",
     "start_time": "2022-12-04T06:25:48.495311Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a89e79",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "23110be2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T19:03:02.205469Z",
     "start_time": "2022-12-07T19:03:02.202602Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_global_configuration():\n",
    "    \"\"\" Retrieve configuration of the training process. \"\"\"\n",
    "\n",
    "    global_config = {\n",
    "      \"num_layers_to_add\": 5,\n",
    "    }\n",
    "\n",
    "    return global_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "887fbc58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T21:05:55.758363Z",
     "start_time": "2022-12-07T21:05:55.754660Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_configuration():\n",
    "    \"\"\" Retrieve configuration for the model. \"\"\"\n",
    "\n",
    "    model_config = {\n",
    "      \"width\": 32,\n",
    "      \"height\": 32,\n",
    "      \"channels\": 3,\n",
    "      \"num_classes\": 10,\n",
    "      \"batch_size\": 250,\n",
    "      \"loss_function\": nn.CrossEntropyLoss,\n",
    "      \"optimizer\": torch.optim.Adam,\n",
    "      \"num_epochs\": 3,\n",
    "      \"hidden_layer_dim\": 256,\n",
    "    }\n",
    "\n",
    "    return model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a903a0",
   "metadata": {},
   "source": [
    "# NN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "2e737ea5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:35:50.555545Z",
     "start_time": "2022-12-07T18:35:50.543732Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerConfigurableNN(nn.Module):\n",
    "    '''\n",
    "    Layer-wise configurable NN\n",
    "    '''\n",
    "    def __init__(self, added_layers = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Retrieve model configuration\n",
    "        self.config = get_model_configuration()\n",
    "        self.width, self.height, self.channels = config.get(\"width\"), config.get(\"height\"), config.get(\"channels\")\n",
    "        self.flatten_shape = config.get(\"width\") * config.get(\"height\") * config.get(\"channels\")\n",
    "        self.layer_dim = config.get(\"layer_dim\")\n",
    "        self.num_classes = config.get(\"num_classes\")\n",
    "\n",
    "        # Create layer structure\n",
    "        layers = self.init_layers()\n",
    "        \n",
    "        for i in range(added_layers):\n",
    "            self.add_layer()\n",
    "\n",
    "        # Create output layers\n",
    "        for layer in self.get_output_layers():\n",
    "            layers.append((str(len(layers)), layer))\n",
    "\n",
    "        # Initialize the Sequential structure\n",
    "        self.layers = nn.Sequential(OrderedDict(layers))\n",
    "    \n",
    "    def init_layers(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_intermediate_layers(self):\n",
    "        raise NotImplementedError \n",
    "        \n",
    "    def get_output_layer(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "\n",
    "    def set_structure(self, layers):\n",
    "        self.layers = nn.Sequential(OrderedDict(layers))\n",
    "        \n",
    "    def num_weights(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def num_trainable_weights(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def add_layer(self):\n",
    "        \"\"\" Add a new layer to a model, setting all others to nontrainable. \"\"\"\n",
    "        config = get_model_configuration()\n",
    "\n",
    "        # Retrieve current layers\n",
    "        layers = self.layers\n",
    "        print(\"=\"*50)\n",
    "        print(\"Old structure:\")\n",
    "        print(layers)\n",
    "\n",
    "        # Save last layer for adding later\n",
    "        last_layer = layers[-1]\n",
    "\n",
    "        # Define new structure\n",
    "        new_structure = []\n",
    "\n",
    "        # Iterate over all except last layer\n",
    "        for layer_index in range(len(layers) - 1):\n",
    "\n",
    "            # For old layer, set all parameters to nontrainable\n",
    "            old_layer = layers[layer_index]\n",
    "            for param in old_layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            # Append old layer to new structure\n",
    "            new_structure.append((str(layer_index), old_layer))\n",
    "\n",
    "        # Append new layer to the final intermediate layer\n",
    "        new_layers = self.get_intermediate_layers()\n",
    "        for layer in new_layers:\n",
    "            new_structure.append((str(len(new_structure)), layer))\n",
    "\n",
    "        # Re-add last layer\n",
    "        new_structure.append((str(len(new_structure)), last_layer))\n",
    "\n",
    "        # Change the model structure\n",
    "        self.set_structure(new_structure)\n",
    "\n",
    "        # Return the model\n",
    "        print(\"=\"*50)\n",
    "        print(\"New structure:\")\n",
    "        print(self.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a124702c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:22:36.386698Z",
     "start_time": "2022-12-07T18:22:36.384076Z"
    }
   },
   "outputs": [],
   "source": [
    "# The images in CIFAR-10 are of size 3x32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "edea6d36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:37:20.834102Z",
     "start_time": "2022-12-07T18:37:20.828006Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerConfigurableCNN(LayerConfigurableNN):\n",
    "    '''\n",
    "    Layer-wise configurable CNN.\n",
    "    '''\n",
    "    def __init__(self, added_layers = 0):\n",
    "        self.out_channels = 6\n",
    "        self.init_kernel_size = 8\n",
    "        self.hidden_kernel_size = 4\n",
    "        self.mp_layers = 1 # max pool layers\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "    def init_layers(self):\n",
    "        return [(str(0), nn.Conv2d(self.channels, self.out_channels, self.init_kernel_size)), \n",
    "                (str(1), nn.MaxPool2d(2)), \n",
    "                (str(2), nn.ReLU())]\n",
    "        \n",
    "    def get_intermediate_layers(self):\n",
    "        return [nn.Conv2d(self.out_channels, self.out_channels, self.hidden_kernel_size), nn.ReLU()]\n",
    "    \n",
    "    def get_output_layers(self):\n",
    "        self.flatten_out_shape = int(self.out_channels * (self.width / (2 * self.mp_layers) - self.hidden_kernel_size\n",
    "            ) * (self.height / (2 * self.mp_layers) - self.hidden_kernel_size))\n",
    "        \n",
    "        return [nn.Flatten(), nn.Linear(self.flatten_out_shape, self.num_classes)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "edb4376b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:37:21.217131Z",
     "start_time": "2022-12-07T18:37:21.211175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerConfigurableCNN(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(3, 6, kernel_size=(8, 8), stride=(1, 1))\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (2): ReLU()\n",
       "    (3): Flatten(start_dim=1, end_dim=-1)\n",
       "    (4): Linear(in_features=864, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LayerConfigurableCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9611b06d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:22:37.050543Z",
     "start_time": "2022-12-07T18:22:37.045771Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerConfigurableMLP(LayerConfigurableNN):\n",
    "    '''\n",
    "    Layer-wise configurable Multilayer Perceptron.\n",
    "    '''\n",
    "    def __init__(self, added_layers = 0):\n",
    "        super().__init__()\n",
    "\n",
    "    def init_layers(self):\n",
    "        return [\n",
    "          (str(0), nn.Flatten()),\n",
    "          (str(1), nn.Linear(self.flatten_shape, self.layer_dim)),\n",
    "          (str(2), nn.ReLU())\n",
    "        ]\n",
    "\n",
    "        \n",
    "    def get_intermediate_layers(self):\n",
    "        return [nn.Linear(self.layer_dim, self.layer_dim), nn.ReLU()]\n",
    "    \n",
    "    def get_output_layers(self):\n",
    "        return [nn.Linear(self.layer_dim, self.num_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ac30850c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:17:10.706889Z",
     "start_time": "2022-12-07T18:17:09.152736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = CIFAR10(os.getcwd(), download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a21acff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:17:10.714548Z",
     "start_time": "2022-12-07T18:17:10.709678Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset(train=True, invariant=False):\n",
    "    \"\"\" Load and convert dataset into inputs and targets \"\"\"\n",
    "    config = get_model_configuration()\n",
    "    if invariant:\n",
    "        T = transforms.Compose([\n",
    "            transforms.RandomChoice([transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip()]),\n",
    "            transforms.RandomRotation((0, 360)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    else:\n",
    "        T = transforms.ToTensor()\n",
    "    dataset = CIFAR10(os.getcwd(), train=train, download=True, transform=T)\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=config.get(\"batch_size\"), shuffle=True, num_workers=1)\n",
    "\n",
    "    return trainloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b73c6",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "224b732c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-05T16:10:58.342928Z",
     "start_time": "2022-12-05T16:10:49.920336Z"
    }
   },
   "outputs": [],
   "source": [
    "output_data = []\n",
    "targets_data = []\n",
    "config = get_model_configuration()\n",
    "model = LayerConfigurableMLP()\n",
    "loss_function = config.get(\"loss_function\")()\n",
    "\n",
    "for i, data in enumerate(testloader):\n",
    "    inputs, targets = data\n",
    "    \n",
    "    # Perform forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, targets)\n",
    "    current_loss += loss.item()\n",
    "    output_data.extend(outputs.detach().numpy())\n",
    "    targets_data.extend(targets.detach().numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5cace0",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "98ac073f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:22:41.688773Z",
     "start_time": "2022-12-07T18:22:41.638739Z"
    }
   },
   "outputs": [],
   "source": [
    "m1 = LayerConfigurableCNN()\n",
    "m2 = LayerConfigurableMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "28a292b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:22:57.553155Z",
     "start_time": "2022-12-07T18:22:57.549349Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Old structure:\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 6, kernel_size=(8, 8), stride=(1, 1))\n",
      "  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (2): ReLU()\n",
      "  (3): Flatten(start_dim=1, end_dim=-1)\n",
      "  (4): Conv2d(6, 6, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=96, out_features=10, bias=True)\n",
      ")\n",
      "==================================================\n",
      "New structure:\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 6, kernel_size=(8, 8), stride=(1, 1))\n",
      "  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (2): ReLU()\n",
      "  (3): Flatten(start_dim=1, end_dim=-1)\n",
      "  (4): Conv2d(6, 6, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): Conv2d(6, 6, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=96, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m1.add_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f2bbba7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:22:59.043691Z",
     "start_time": "2022-12-07T18:22:59.038751Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1552"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.num_trainable_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e86169b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:23:23.275120Z",
     "start_time": "2022-12-07T18:23:23.263235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Old structure:\n",
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=3072, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "==================================================\n",
      "New structure:\n",
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=3072, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m2.add_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6263ab8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:23:25.196083Z",
     "start_time": "2022-12-07T18:23:25.191207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68362"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.num_trainable_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ee82a6db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:21:45.337923Z",
     "start_time": "2022-12-07T18:21:45.333158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Old structure:\n",
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=3072, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "==================================================\n",
      "New structure:\n",
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=3072, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "m2.num_weights()\n",
    "m2.add_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "025798c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T18:21:53.742718Z",
     "start_time": "2022-12-07T18:21:53.738703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68362"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.num_trainable_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691c4db",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "d1312957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T21:05:58.424995Z",
     "start_time": "2022-12-07T21:05:58.406982Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top1_pos(outputs, targets):\n",
    "    pred = np.argmax(outputs, axis=1)\n",
    "    assert(len(pred) == len(targets))\n",
    "    \n",
    "    return np.sum(np.where(pred == targets, 1, 0))\n",
    "\n",
    "def get_top5_pos(outputs, targets):\n",
    "    sm = 0\n",
    "    for i in range(len(targets)):\n",
    "        top_5 = np.argpartition(outputs[i], -5)[-5:]\n",
    "        sm += 1 if targets[i] in set(top_5) else 0 \n",
    "    \n",
    "    return sm\n",
    "\n",
    "def test_model(model, loss_function):\n",
    "    testloader = get_dataset(train=False)\n",
    "    \n",
    "    output_data = []\n",
    "    targets_data = []\n",
    "    current_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(testloader):\n",
    "        inputs, targets = data\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        output_data.extend(outputs.detach().numpy())\n",
    "        targets_data.extend(targets.detach().numpy())\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        current_loss += loss.item()\n",
    "        \n",
    "    N = len(targets_data)\n",
    "    top1_acc = get_top1_pos(output_data, targets_data) / N\n",
    "    top5_acc = get_top5_pos(output_data, targets_data) / N\n",
    "    \n",
    "    return current_loss, top1_acc, top5_acc\n",
    "\n",
    "def train_model(model, epochs=None, debug=False):\n",
    "    \"\"\" Train a model. \"\"\"\n",
    "    config = get_model_configuration()\n",
    "    loss_function = config.get(\"loss_function\")()\n",
    "    optimizer = config.get(\"optimizer\")(model.parameters(), lr=1e-4)\n",
    "    trainloader = get_dataset()\n",
    "    accelerator = Accelerator()  \n",
    "\n",
    "    # Accelerate model\n",
    "    model, optimizer, trainloader = accelerator.prepare(model, optimizer, trainloader)\n",
    "\n",
    "    # Iterate over the number of epochs\n",
    "    entries = []\n",
    "    \n",
    "    if epochs is None:\n",
    "        epochs = config.get(\"num_epochs\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "        \n",
    "        # Positive / Accuracy Rate\n",
    "        top_1_positives = 0 \n",
    "        top_5_positives = 0\n",
    "        n = 0\n",
    "        \n",
    "        output_data = []\n",
    "        targets_data = []\n",
    " \n",
    "        # Iterate over the DataLoader for training data\n",
    "        st_time = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "#             print(i)\n",
    "\n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Perform forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "#             print(outputs)\n",
    "#             print(outputs.shape)\n",
    "#             print(targets)\n",
    "#             print(targets.shape)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            \n",
    "#             o, t = outputs.detach().numpy(), targets.detach().numpy()\n",
    "#             top_1_positives += get_top1_pos(o, t)\n",
    "#             top_5_positives += get_top5_pos(o, t)\n",
    "#             n += len(targets)\n",
    "\n",
    "            output_data.extend(outputs.cpu().detach().numpy())\n",
    "            targets_data.extend(targets.cpu().detach().numpy())\n",
    "            current_loss += loss.item()\n",
    "            \n",
    "            # Perform backward pass\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics\n",
    "            if debug:\n",
    "                print('Loss after mini-batch %5d: %.3f' %\n",
    "                    (i + 1, current_loss / 500))\n",
    "#             end_loss = current_loss / 500\n",
    "#             current_loss = 0.0\n",
    "\n",
    "        end_time = time.time()\n",
    "    \n",
    "        top1_acc = get_top1_pos(output_data, targets_data) / len(targets_data)\n",
    "        top5_acc = get_top5_pos(output_data, targets_data) / len(targets_data)\n",
    "        \n",
    "        train_entry = {'type': 'train', 'epoch': epoch, 'top1': top1_acc, 'top5': top5_acc,\n",
    "                       'loss': current_loss, 'time': round(end_time - st_time, 1)}\n",
    "        \n",
    "        test_st_time = time.time()\n",
    "        test_loss, test_top1_acc, test_top5_acc = test_model(model, loss_function)\n",
    "        test_end_time = time.time()\n",
    "        \n",
    "        print(f'Loss: {current_loss}')\n",
    "        print(f'Train Acc: {top1_acc}')\n",
    "        print(f'Test Acc: {test_top1_acc}')\n",
    "        \n",
    "        test_entry = {'type': 'test', 'epoch': epoch, 'top1': test_top1_acc, 'top5': test_top5_acc,\n",
    "                      'loss': test_loss, 'time': round(test_st_time - test_end_time, 1)}\n",
    "        \n",
    "        entries.extend([train_entry, test_entry])\n",
    "\n",
    "    print(n)\n",
    "    print(top_1_positives)\n",
    "    print(top_5_positives)\n",
    "\n",
    "    # Return trained model\n",
    "    return model, pd.DataFrame(entries), current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "bb4b7b35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T21:07:46.643313Z",
     "start_time": "2022-12-07T21:06:40.116135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Starting epoch 1\n",
      "Files already downloaded and verified\n",
      "Loss: 399.7074911594391\n",
      "Train Acc: 0.29368\n",
      "Test Acc: 0.3426\n",
      "Starting epoch 2\n",
      "Files already downloaded and verified\n",
      "Loss: 369.48126745224\n",
      "Train Acc: 0.35666\n",
      "Test Acc: 0.3582\n",
      "Starting epoch 3\n",
      "Files already downloaded and verified\n",
      "Loss: 357.95565962791443\n",
      "Train Acc: 0.37596\n",
      "Test Acc: 0.3896\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [304], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mlp \u001b[38;5;241m=\u001b[39m LayerConfigurableMLP()\n\u001b[0;32m----> 2\u001b[0m mlp, mlp_df \u001b[38;5;241m=\u001b[39m train_model(mlp)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "mlp = LayerConfigurableMLP()\n",
    "mlp, mlp_df = train_model(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ad8b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T20:59:29.147743Z",
     "start_time": "2022-12-07T20:59:29.147729Z"
    }
   },
   "outputs": [],
   "source": [
    "# cnn = LayerConfigurableCNN()\n",
    "# cnn, cnn_df = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f764a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T20:59:29.149013Z",
     "start_time": "2022-12-07T20:59:29.149000Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def greedy_layerwise_training(model):\n",
    "    \"\"\" Perform greedy layer-wise training. \"\"\"\n",
    "    \n",
    "    print(\"NEW!\")\n",
    "    global_config = get_global_configuration()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Loss comparison\n",
    "    loss_comparable = float('inf')\n",
    "\n",
    "    # Iterate over the number of layers to add\n",
    "    training_losses = []\n",
    "    top5_accs = []\n",
    "    top1_accs = []\n",
    "    \n",
    "    dfs = []\n",
    "    for num_layers in range(global_config.get(\"num_layers_to_add\")):\n",
    "        # Print which model is trained\n",
    "        print(\"=\"*100)\n",
    "        if num_layers > 0:\n",
    "            print(f\">>> TRAINING THE MODEL WITH {num_layers} ADDITIONAL LAYERS:\")\n",
    "        else:\n",
    "            print(f\">>> TRAINING THE BASE MODEL:\")\n",
    "\n",
    "        # Train the model\n",
    "        model, df, end_loss = train_model(model)\n",
    "        df['layer'] = num_layers\n",
    "        dfs.append(df)\n",
    "\n",
    "        # Compare loss\n",
    "        if num_layers > 0 and end_loss < loss_comparable:\n",
    "            print(\"=\"*50)\n",
    "            print(f\">>> RESULTS: Adding this layer has improved the model loss from {loss_comparable} to {end_loss}\")\n",
    "            loss_comparable = end_loss\n",
    "        elif num_layers > 0:\n",
    "            print(\"=\"*50)\n",
    "            print(f\">>> RESULTS: Adding this layer did not improve the model loss from {loss_comparable} to {end_loss}\")\n",
    "        elif num_layers == 0:\n",
    "            loss_comparable = end_loss\n",
    "\n",
    "        # Add layer to model\n",
    "        model.add_layer()\n",
    "\n",
    "    # Process is complete\n",
    "    print(\"Training process has finished.\")\n",
    "    \n",
    "    return model, pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "8dbf39b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T20:53:53.388799Z",
     "start_time": "2022-12-07T20:53:53.367421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>epoch</th>\n",
       "      <th>top1</th>\n",
       "      <th>top5</th>\n",
       "      <th>loss</th>\n",
       "      <th>time</th>\n",
       "      <th>layer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0.37734</td>\n",
       "      <td>0.84736</td>\n",
       "      <td>357.612984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>0.38280</td>\n",
       "      <td>0.85650</td>\n",
       "      <td>70.794490</td>\n",
       "      <td>13.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0.38296</td>\n",
       "      <td>0.86356</td>\n",
       "      <td>348.311132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>0.38910</td>\n",
       "      <td>0.86480</td>\n",
       "      <td>69.314775</td>\n",
       "      <td>12.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0.38516</td>\n",
       "      <td>0.86536</td>\n",
       "      <td>345.427758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>0.38800</td>\n",
       "      <td>0.86640</td>\n",
       "      <td>68.870548</td>\n",
       "      <td>12.7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0.38208</td>\n",
       "      <td>0.86618</td>\n",
       "      <td>344.685656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>0.37950</td>\n",
       "      <td>0.86830</td>\n",
       "      <td>68.841312</td>\n",
       "      <td>12.7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>0.38164</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>344.831558</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>0.37770</td>\n",
       "      <td>0.86620</td>\n",
       "      <td>68.858117</td>\n",
       "      <td>13.4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  epoch     top1     top5        loss  time  layer\n",
       "4  train      2  0.37734  0.84736  357.612984   NaN      0\n",
       "5   test      2  0.38280  0.85650   70.794490  13.3      0\n",
       "4  train      2  0.38296  0.86356  348.311132   NaN      1\n",
       "5   test      2  0.38910  0.86480   69.314775  12.9      1\n",
       "4  train      2  0.38516  0.86536  345.427758   NaN      2\n",
       "5   test      2  0.38800  0.86640   68.870548  12.7      2\n",
       "4  train      2  0.38208  0.86618  344.685656   NaN      3\n",
       "5   test      2  0.37950  0.86830   68.841312  12.7      3\n",
       "4  train      2  0.38164  0.86630  344.831558   NaN      4\n",
       "5   test      2  0.37770  0.86620   68.858117  13.4      4"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df['epoch'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "eb83b55c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T20:59:29.135182Z",
     "start_time": "2022-12-07T20:55:07.199454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW!\n",
      "====================================================================================================\n",
      ">>> TRAINING THE BASE MODEL:\n",
      "Files already downloaded and verified\n",
      "Starting epoch 1\n",
      "Files already downloaded and verified\n",
      "Loss: 399.98213946819305\n",
      "Train Acc: 0.29576\n",
      "Test Acc: 0.3287\n",
      "Starting epoch 2\n",
      "Files already downloaded and verified\n",
      "Loss: 369.30824625492096\n",
      "Train Acc: 0.35532\n",
      "Test Acc: 0.3654\n",
      "Starting epoch 3\n",
      "Files already downloaded and verified\n",
      "Loss: 357.61298418045044\n",
      "Train Acc: 0.37734\n",
      "Test Acc: 0.3828\n",
      "Starting epoch 4\n",
      "Files already downloaded and verified\n",
      "Loss: 349.37613356113434\n",
      "Train Acc: 0.38926\n",
      "Test Acc: 0.3958\n",
      "Starting epoch 5\n",
      "Files already downloaded and verified\n",
      "Loss: 342.0980815887451\n",
      "Train Acc: 0.4056\n",
      "Test Acc: 0.4066\n",
      "Starting epoch 6\n",
      "Files already downloaded and verified\n",
      "Loss: 335.85399198532104\n",
      "Train Acc: 0.41578\n",
      "Test Acc: 0.409\n",
      "Starting epoch 7\n",
      "Files already downloaded and verified\n",
      "Loss: 330.08105981349945\n",
      "Train Acc: 0.42612\n",
      "Test Acc: 0.4292\n",
      "Starting epoch 8\n",
      "Files already downloaded and verified\n",
      "Loss: 325.2761056423187\n",
      "Train Acc: 0.43522\n",
      "Test Acc: 0.429\n",
      "Starting epoch 9\n",
      "Files already downloaded and verified\n",
      "Loss: 320.99711084365845\n",
      "Train Acc: 0.4431\n",
      "Test Acc: 0.4383\n",
      "Starting epoch 10\n",
      "Files already downloaded and verified\n",
      "Loss: 317.31901133060455\n",
      "Train Acc: 0.44756\n",
      "Test Acc: 0.4467\n",
      "Starting epoch 11\n",
      "Files already downloaded and verified\n",
      "Loss: 313.6143901348114\n",
      "Train Acc: 0.4552\n",
      "Test Acc: 0.4525\n",
      "Starting epoch 12\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [295], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mlp_model, results_df \u001b[38;5;241m=\u001b[39m \u001b[43mgreedy_layerwise_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLayerConfigurableMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [289], line 26\u001b[0m, in \u001b[0;36mgreedy_layerwise_training\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> TRAINING THE BASE MODEL:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m model, df, end_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m num_layers\n\u001b[1;32m     28\u001b[0m dfs\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "Cell \u001b[0;32mIn [292], line 99\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, debug)\u001b[0m\n\u001b[1;32m     95\u001b[0m train_entry \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop1\u001b[39m\u001b[38;5;124m'\u001b[39m: top1_acc, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop5\u001b[39m\u001b[38;5;124m'\u001b[39m: top5_acc,\n\u001b[1;32m     96\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: current_loss, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mround\u001b[39m(end_time \u001b[38;5;241m-\u001b[39m st_time, \u001b[38;5;241m1\u001b[39m)}\n\u001b[1;32m     98\u001b[0m test_st_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 99\u001b[0m test_loss, test_top1_acc, test_top5_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m test_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [147], line 8\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, loss_function)\u001b[0m\n\u001b[1;32m      5\u001b[0m targets_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m current_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(testloader):\n\u001b[1;32m      9\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Perform forward pass\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1305\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;66;03m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1305\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shutdown_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;66;03m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \n\u001b[1;32m   1310\u001b[0m \u001b[38;5;66;03m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1430\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers:\n\u001b[1;32m   1427\u001b[0m     \u001b[38;5;66;03m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m     \u001b[38;5;66;03m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m     \u001b[38;5;66;03m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1430\u001b[0m     \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMP_STATUS_CHECK_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues:\n\u001b[1;32m   1432\u001b[0m     q\u001b[38;5;241m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.10/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentinel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.10/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/.conda/envs/pytorch/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp_model, results_df = greedy_layerwise_training(LayerConfigurableMLP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b35fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-07T19:56:34.903574Z",
     "start_time": "2022-12-07T19:56:34.903556Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_model, results_df = greedy_layerwise_training(LayerConfigurableCNN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d38c0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
