{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5c4824fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (0.15.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from accelerate) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from accelerate) (1.21.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from packaging>=20.0->accelerate) (3.0.7)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ece8329e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T01:51:40.512881Z",
     "start_time": "2022-12-10T01:51:40.498991Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4f44bf72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T01:51:41.010498Z",
     "start_time": "2022-12-10T01:51:41.007161Z"
    }
   },
   "outputs": [],
   "source": [
    "# import accelerate\n",
    "import models\n",
    "import importlib\n",
    "import helper\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "653dad35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T01:51:41.467908Z",
     "start_time": "2022-12-10T01:51:41.464151Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3af53f23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T01:51:41.863963Z",
     "start_time": "2022-12-10T01:51:41.860644Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "36f525d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T01:51:42.693036Z",
     "start_time": "2022-12-10T01:51:42.690037Z"
    }
   },
   "outputs": [],
   "source": [
    "# workdir = '/Users/erichansen/Desktop/Classes/9.520/project/'\n",
    "workdir = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34484985",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "223ba8aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T01:51:44.089496Z",
     "start_time": "2022-12-10T01:51:44.085074Z"
    }
   },
   "outputs": [],
   "source": [
    "global_config = config.get_global_configuration()\n",
    "device = global_config['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d208f808",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-11T00:10:04.362257Z",
     "start_time": "2022-12-11T00:10:04.255261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/home/ec2-user/SageMaker/layerwise-nn-optimization/config.py'>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(models)\n",
    "importlib.reload(helper)\n",
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2568043b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T02:27:49.251388Z",
     "start_time": "2022-12-10T02:27:48.812232Z"
    }
   },
   "outputs": [],
   "source": [
    "# m1 = models.LayerwiseConfigurableCNN()\n",
    "m2 = models.LayerwiseConfigurableMLP(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e5773",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7abeb0a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T03:33:37.434644Z",
     "start_time": "2022-12-10T03:33:37.421496Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top1_pos(outputs, targets):\n",
    "    pred = np.argmax(outputs, axis=1)\n",
    "    assert(len(pred) == len(targets))\n",
    "    \n",
    "    return np.sum(np.where(pred == targets, 1, 0))\n",
    "\n",
    "def get_top5_pos(outputs, targets):\n",
    "    sm = 0\n",
    "    for i in range(len(targets)):\n",
    "        top_5 = np.argpartition(outputs[i], -5)[-5:]\n",
    "        sm += 1 if targets[i] in set(top_5) else 0 \n",
    "    \n",
    "    return sm\n",
    "\n",
    "def evaluate_model(model, data_loader, loss_function, device='cpu'):    \n",
    "    output_data = []\n",
    "    targets_data = []\n",
    "    current_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if str(device) != 'cpu':\n",
    "            outputs = outputs.cpu()\n",
    "            targets = targets.cpu()\n",
    "            \n",
    "        output_data.extend(outputs.detach().numpy())\n",
    "        targets_data.extend(targets.detach().numpy())\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        current_loss += loss.item()\n",
    "        \n",
    "    N = len(targets_data)\n",
    "    top1_acc = get_top1_pos(output_data, targets_data) / N\n",
    "    top5_acc = get_top5_pos(output_data, targets_data) / N\n",
    "    \n",
    "    return current_loss / float(N), top1_acc, top5_acc    \n",
    "\n",
    "\n",
    "def train_model(model, device='cpu', epochs=None, invariant=False, output=False):\n",
    "    \"\"\" Train a model. \"\"\"\n",
    "    model_config = config.get_model_configuration()\n",
    "    print(device)\n",
    "    \n",
    "    loss_function = model_config.get(\"loss_function\")()\n",
    "    optimizer = model_config.get(\"optimizer\")(model.parameters(), \n",
    "                                              lr=model_config.get('learning_rate'),\n",
    "                                              weight_decay=model_config.get('weight_decay'))\n",
    "    trainloader = helper.get_dataset(train=True, invariant=invariant)\n",
    "    testloader = helper.get_dataset(train=False, invariant=invariant)\n",
    "\n",
    "#     Accelerate model\n",
    "#     accelerator = accelerate.Accelerator()  \n",
    "#     model, optimizer, trainloader = accelerator.prepare(model, optimizer, trainloader)\n",
    "\n",
    "    # Iterate over the number of epochs\n",
    "    entries = []\n",
    "    \n",
    "    if epochs is None:\n",
    "        epochs = model_config.get(\"num_epochs\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Print epoch\n",
    "        if output:\n",
    "            print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "        \n",
    "        output_data = []\n",
    "        targets_data = []\n",
    " \n",
    "        # Iterate over the DataLoader for training data\n",
    "        st_time = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "#             print(i)\n",
    "\n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "\n",
    "            current_loss += loss.item()\n",
    "            \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if (epoch % 1 == 0) or (epoch == (epochs - 1)):\n",
    "            test_loss, test_top1_acc, test_top5_acc = evaluate_model(model, testloader ,loss_function, device=device)\n",
    "            train_loss, train_top1_acc, train_top5_acc = evaluate_model(model, trainloader ,loss_function, device=device)\n",
    "            if output:\n",
    "                print(f'Train Acc: {train_top1_acc}')\n",
    "                print(f'Test Acc: {test_top1_acc}')\n",
    "        else:\n",
    "            test_loss, test_top1_acc, test_top5_acc = pd.NA, pd.NA, pd.NA\n",
    "            train_loss, train_top1_acc, train_top5_acc = pd.NA, pd.NA, pd.NA\n",
    "        \n",
    "        elapsed_time = round(end_time - st_time, 1)\n",
    "        train_entry = {'type': 'train', 'epoch': epoch, 'top1': train_top1_acc, 'top5': train_top5_acc,\n",
    "                       'loss': train_loss, 'time': elapsed_time,\n",
    "                       'grad_norm': model.gradient_norm(), 'network_norm': model.network_norm()\n",
    "                      }\n",
    "        \n",
    "        if output:\n",
    "            print(f'Loss: {current_loss}')\n",
    "            print(f'Time: {elapsed_time}')\n",
    "        \n",
    "        test_entry = {'type': 'test', 'epoch': epoch, 'top1': test_top1_acc, 'top5': test_top5_acc,\n",
    "                      'loss': test_loss, 'time': pd.NA}\n",
    "        \n",
    "        entries.extend([train_entry, test_entry])\n",
    "        \n",
    "#         break\n",
    "\n",
    "\n",
    "    # Return trained model\n",
    "    return model, pd.DataFrame(entries), current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53eab03e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T03:33:38.550582Z",
     "start_time": "2022-12-10T03:33:38.545863Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_config_columns(model, strategy, results_df):\n",
    "    model_config = config.get_model_configuration()\n",
    "    global_config = config.get_global_configuration()\n",
    "    \n",
    "    results_df['optimizer'] = str(model_config['optimizer'])\n",
    "    results_df['hidden_layer_dim'] = model_config['hidden_layer_dim']\n",
    "    results_df['batch_size'] = model_config['batch_size']\n",
    "    results_df['batch_norm'] = model_config['batch_norm']\n",
    "    results_df['weight_decay'] = model_config['weight_decay']\n",
    "    results_df['learning_rate'] = model_config['learning_rate']\n",
    "    results_df['invariant'] = global_config['invariant']\n",
    "    results_df['condition'] = global_config['condition']\n",
    "    results_df['max_epochs'] = model_config['num_epochs']\n",
    "    \n",
    "    results_df['model'] = model.get_name()\n",
    "    results_df['train_strategy'] = strategy\n",
    "    results_df['model_strategy'] = results_df['model'] + '_' + results_df['train_strategy']\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4dfbf12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T03:33:38.762117Z",
     "start_time": "2022-12-10T03:33:38.758755Z"
    }
   },
   "outputs": [],
   "source": [
    "def prep_model(model, num_layers):\n",
    "    while len(model.hidden_blocks) + 1 < num_layers:\n",
    "        model.add_hidden_block(device)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98ec8152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T03:33:38.970038Z",
     "start_time": "2022-12-10T03:33:38.960460Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def greedy_layerwise_training(model, output=False):\n",
    "    \"\"\" Perform greedy layer-wise training. \"\"\"    \n",
    "    global_config = config.get_global_configuration()\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    rnds = global_config['rounds']\n",
    "    device = global_config.get('device')\n",
    "    num_layers = global_config.get(\"num_layers\")\n",
    "\n",
    "    # Loss comparison\n",
    "    loss_comparable = float('inf')\n",
    "    \n",
    "    dfs = []\n",
    "\n",
    "    model = prep_model(model, num_layers)\n",
    "    model.freeze_layers([model.input_block] + model.hidden_blocks)\n",
    "\n",
    "    for rnd in range(rnds):\n",
    "        t0 = time.time()\n",
    "        print(f\"Round: {rnd}\")\n",
    "        for layer_num in range(num_layers):\n",
    "            active_block = model.input_block if layer_num == 0 else model.hidden_blocks[layer_num - 1]\n",
    "            model.activate_layer(active_block)\n",
    "            \n",
    "            # Print which model is trained\n",
    "            if output:\n",
    "                print(\"=\"*100)\n",
    "                if num_layers > 0:\n",
    "                    print(f\">>> TRAINING THE MODEL WITH {num_layers} ADDITIONAL LAYERS:\")\n",
    "                else:\n",
    "                    print(f\">>> TRAINING THE BASE MODEL:\")\n",
    "\n",
    "            # Train the model\n",
    "            model, df, end_loss = train_model(model, device=device, invariant=global_config['invariant'])\n",
    "            df['layer'] = layer_num\n",
    "            df['round'] = rnd\n",
    "            \n",
    "            trainable_weights = model.num_trainable_weights()\n",
    "            df['trainable_params'] = trainable_weights\n",
    "            dfs.append(df)\n",
    "\n",
    "            # Compare loss\n",
    "            if output:\n",
    "                print(f'Num Trainable Weights: {trainable_weights}')\n",
    "                print(f'Expected Trainable Weights: {sum(p.numel() for p in active_block.parameters() if p.requires_grad)}')\n",
    "                if num_layers > 0 and end_loss < loss_comparable:\n",
    "                    print(\"=\"*50)\n",
    "                    print(f\">>> RESULTS: Adding this layer has improved the model loss from {loss_comparable} to {end_loss}\")\n",
    "                elif num_layers > 0:\n",
    "                    print(\"=\"*50)\n",
    "                    print(f\">>> RESULTS: Adding this layer did not improve the model loss from {loss_comparable} to {end_loss}\")\n",
    "            loss_comparable = end_loss\n",
    "            \n",
    "            # Freeze Active Layer\n",
    "            model.freeze_layer(active_block)\n",
    "#             break\n",
    "        t1 = time.time()\n",
    "        elapsed_time = round(t1 - t0, 1)\n",
    "        print(elapsed_time)\n",
    "\n",
    "#         break\n",
    "    # Process is complete\n",
    "    print(\"Training process has finished.\")\n",
    "    \n",
    "    results_df = pd.concat(dfs)\n",
    "    strat = 'layerwise'\n",
    "    results_df = add_config_columns(model, strat, results_df)\n",
    "\n",
    "    results_df.to_csv(workdir + f'results/{helper.get_datetime_str(datetime.datetime.now())}_{model.get_name()}_{strat}.csv')\n",
    "    print('finished')\n",
    "    \n",
    "    return model, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd90f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T03:33:39.226578Z",
     "start_time": "2022-12-10T03:33:39.220345Z"
    }
   },
   "outputs": [],
   "source": [
    "def full_backprop_training(model, output=False, expand_rounds=False):\n",
    "    \"\"\" Perform full backprop training. \"\"\"    \n",
    "    global_config = config.get_global_configuration()\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    device = global_config.get('device')\n",
    "    rnds = global_config['rounds']\n",
    "    num_layers = global_config.get(\"num_layers\")\n",
    "    \n",
    "    model = prep_model(model, num_layers)\n",
    "    # Loss comparison\n",
    "    dfs = []\n",
    "    \n",
    "    rnds = rnds * num_layers if expand_rounds else rnds\n",
    "    for rnd in range(rnds):\n",
    "        print(f\"Round: {rnd}\")\n",
    "        t0 = time.time()\n",
    "        model, df, end_loss = train_model(model, device=device, invariant=global_config['invariant'])\n",
    "        t1 = time.time()\n",
    "        elapsed_time = round(t1 - t0, 1)\n",
    "        print(elapsed_time)\n",
    "        \n",
    "        if output:\n",
    "            print(i)\n",
    "            print(end_loss)\n",
    "\n",
    "        df['trainable_params'] = model.num_trainable_weights()\n",
    "        df['round'] = rnd\n",
    "        dfs.append(df)\n",
    "#         break\n",
    "    \n",
    "    results_df = pd.concat(dfs)\n",
    "    strat = 'backprop'\n",
    "    results_df = add_config_columns(model, strat, results_df)\n",
    "        \n",
    "    results_df.to_csv(workdir + f'results/{helper.get_datetime_str(datetime.datetime.now())}_{model.get_name()}_{strat}.csv')\n",
    "    print('finished')\n",
    "    \n",
    "    return model, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a13f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T03:33:39.787523Z",
     "start_time": "2022-12-10T03:33:39.784476Z"
    }
   },
   "outputs": [],
   "source": [
    "def last_epoch_df(df):\n",
    "#     def last_epoch_grp(grp):\n",
    "#         return grp[grp['epoch'] == grp['epoch'].max()]\n",
    "    return df.groupby(by=['model', 'rnd', 'train_strategy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d91e2",
   "metadata": {},
   "source": [
    "# Run Training Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ed1de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "656e1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simplex_etf(in_dim, out_dim, seed=520):\n",
    "    print(in_dim)\n",
    "    print(out_dim)\n",
    "\n",
    "    M = np.sqrt(out_dim / (out_dim - 1)) * np.identity(out_dim) - \\\n",
    "        (1 / float(out_dim)) * np.ones((out_dim, out_dim))\n",
    "    U = scipy.stats.ortho_group.rvs(in_dim, random_state=seed)[:, :out_dim]\n",
    "\n",
    "    ETF = np.matmul(U, M)\n",
    "\n",
    "    return torch.from_numpy(ETF.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1eb41d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(CNN16_simplex_etf, './CNN16_simplex_etf.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92e3c881",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-11T00:30:06.468Z"
    }
   },
   "outputs": [],
   "source": [
    "# mlp_model = models.LayerwiseConfigurableMLP(device)\n",
    "# mlp_model, mlp_results_df = greedy_layerwise_training(mlp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef6728c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-10T04:20:56.822944Z",
     "start_time": "2022-12-10T04:15:47.656606Z"
    }
   },
   "outputs": [],
   "source": [
    "# mlp_bp_model = models.LayerwiseConfigurableMLP(device)\n",
    "# mlp_bp_model, mlp_bp_results_df = full_backprop_training(mlp_bp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3fdb2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "etf = torch.load('./CNN16_simplex_etf.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "21c42f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "569.7\n",
      "Round: 1\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "565.8\n",
      "Round: 2\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "566.3\n",
      "Round: 3\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "566.3\n",
      "Round: 4\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "565.6\n",
      "Training process has finished.\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "cnn_model = models.LayerwiseConfigurableCNN(device)\n",
    "cnn_model, cnn_results_df = greedy_layerwise_training(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round: 0\n",
      "cuda:0\n",
      "190.1\n",
      "Round: 1\n",
      "cuda:0\n",
      "189.6\n",
      "Round: 2\n",
      "cuda:0\n",
      "190.1\n",
      "Round: 3\n",
      "cuda:0\n",
      "189.3\n",
      "Round: 4\n",
      "cuda:0\n",
      "191.3\n",
      "Round: 5\n",
      "cuda:0\n",
      "190.4\n",
      "Round: 6\n",
      "cuda:0\n",
      "189.9\n",
      "Round: 7\n",
      "cuda:0\n",
      "189.2\n",
      "Round: 8\n",
      "cuda:0\n",
      "189.0\n",
      "Round: 9\n",
      "cuda:0\n",
      "189.4\n",
      "Round: 10\n",
      "cuda:0\n",
      "187.8\n",
      "Round: 11\n",
      "cuda:0\n",
      "187.8\n",
      "Round: 12\n",
      "cuda:0\n",
      "189.5\n",
      "Round: 13\n",
      "cuda:0\n",
      "188.7\n",
      "Round: 14\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cnn_model = models.LayerwiseConfigurableCNN(device)\n",
    "cnn_model, cnn_results_df = full_backprop_training(cnn_model, expand_rounds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ea07fc",
   "metadata": {},
   "source": [
    "# By Epoch Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfacd42",
   "metadata": {},
   "source": [
    "Questions\n",
    "\n",
    "1) Understanding the curvature of the loss function - how to compute the Hessian\n",
    "2) Should I freeze the output layer?\n",
    "3) What does it mean to set W_L using the neural collapse property?\n",
    "4) Skip connections - does that mean that each layer needs to have the same output dimension as the final output dimension? Do we just sum them up at the end\n",
    "5) Training Resources\n",
    "6) Weight Decay? Batch Normalization?\n",
    "7) Width of Hidden Layers in MLP\n",
    "\n",
    "8) Depth of MLP\n",
    "9) Number of Channels in CNN\n",
    "10) Kernel Size in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd0700f",
   "metadata": {},
   "source": [
    "Lecture\n",
    "\n",
    "1) Understanding, as a mathematician, the critical points of L\n",
    "2) Goal\n",
    "\n",
    "\n",
    "For deep (l >= 3) nonlinear networks, bad local (non-global) minimima exist - that are difficult to escape\n",
    "Morse Function. A function L: R^d -> R is MOrse if at every critical point p in R^d the Hessian Hess(L) (p) is nonsingular (i.e. has no 0 eigenvalues)\n",
    "\n",
    "1) If L is Morse, can understand the topology of u by computing all the critical points of L and geometry near them\n",
    "2) Almost every c^2 function is Morse (Morse functions are open, dense in C^2)\n",
    "\n",
    "Morse-Bott function\n",
    "Allow for non-isolated critical points\n",
    "L: R^d -> R is Morse Bott f critical locus is a closed submanifold and Hess(L) is nonsingular in normal directions to that submanifold\n",
    "\n",
    "Geometry changes significantly across regimes\n",
    "- width > n\n",
    "- width > poly(n)\n",
    "- width > sqrt(n)\n",
    "\n",
    "As soon as there exists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42740890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "324px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
