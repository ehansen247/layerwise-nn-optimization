{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "474a3be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (0.15.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from accelerate) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from accelerate) (1.21.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from packaging>=20.0->accelerate) (3.0.7)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages (from torch>=1.4.0->accelerate) (0.18.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "96df242f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T08:12:46.293823Z",
     "start_time": "2022-12-08T08:12:46.266230Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "22816f1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T08:12:47.212847Z",
     "start_time": "2022-12-08T08:12:47.204468Z"
    }
   },
   "outputs": [],
   "source": [
    "import accelerate\n",
    "import models\n",
    "import importlib\n",
    "import helper\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e7ac06c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T08:12:47.396903Z",
     "start_time": "2022-12-08T08:12:47.386297Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e8536099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T08:12:47.586385Z",
     "start_time": "2022-12-08T08:12:47.580757Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69177085",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "02ea9448",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T16:10:31.735295Z",
     "start_time": "2022-12-08T16:10:31.728658Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "global_config = config.get_global_configuration()\n",
    "device = global_config['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fa472",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-08T16:18:27.319Z"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(models)\n",
    "importlib.reload(helper)\n",
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d3b71ee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T08:15:21.733294Z",
     "start_time": "2022-12-08T08:15:21.717739Z"
    }
   },
   "outputs": [],
   "source": [
    "m1 = models.LayerwiseConfigurableCNN()\n",
    "m2 = models.LayerwiseConfigurableMLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77953e2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7d468c72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T08:29:14.521046Z",
     "start_time": "2022-12-08T08:29:14.500000Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top1_pos(outputs, targets):\n",
    "    pred = np.argmax(outputs, axis=1)\n",
    "    assert(len(pred) == len(targets))\n",
    "    \n",
    "    return np.sum(np.where(pred == targets, 1, 0))\n",
    "\n",
    "def get_top5_pos(outputs, targets):\n",
    "    sm = 0\n",
    "    for i in range(len(targets)):\n",
    "        top_5 = np.argpartition(outputs[i], -5)[-5:]\n",
    "        sm += 1 if targets[i] in set(top_5) else 0 \n",
    "    \n",
    "    return sm\n",
    "\n",
    "def evaluate_model(model, data_loader, loss_function, device='cpu'):    \n",
    "    output_data = []\n",
    "    targets_data = []\n",
    "    current_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if str(device) != 'cpu':\n",
    "            outputs = outputs.cpu()\n",
    "            targets = targets.cpu()\n",
    "            \n",
    "        output_data.extend(outputs.detach().numpy())\n",
    "        targets_data.extend(targets.detach().numpy())\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        current_loss += loss.item()\n",
    "        \n",
    "    N = len(targets_data)\n",
    "    top1_acc = get_top1_pos(output_data, targets_data) / N\n",
    "    top5_acc = get_top5_pos(output_data, targets_data) / N\n",
    "    \n",
    "    return current_loss, top1_acc, top5_acc    \n",
    "\n",
    "\n",
    "def train_model(model, device='cpu', epochs=None, invariant=False, debug=False):\n",
    "    \"\"\" Train a model. \"\"\"\n",
    "    model_config = config.get_model_configuration()\n",
    "    \n",
    "    loss_function = model_config.get(\"loss_function\")()\n",
    "    optimizer = model_config.get(\"optimizer\")(model.parameters(), lr=1e-4)\n",
    "    trainloader = helper.get_dataset(train=True, invariant=invariant)\n",
    "    testloader = helper.get_dataset(train=False, invariant=invariant)\n",
    "\n",
    "#     Accelerate model\n",
    "#     accelerator = accelerate.Accelerator()  \n",
    "#     model, optimizer, trainloader = accelerator.prepare(model, optimizer, trainloader)\n",
    "\n",
    "    # Iterate over the number of epochs\n",
    "    entries = []\n",
    "    \n",
    "    if epochs is None:\n",
    "        epochs = model_config.get(\"num_epochs\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Print epoch\n",
    "        print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "        # Set current loss value\n",
    "        current_loss = 0.0\n",
    "        \n",
    "        output_data = []\n",
    "        targets_data = []\n",
    " \n",
    "        # Iterate over the DataLoader for training data\n",
    "        st_time = time.time()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "#             print(i)\n",
    "\n",
    "            # Get inputs\n",
    "            inputs, targets = data\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Perform forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs, targets)\n",
    "\n",
    "            current_loss += loss.item()\n",
    "            \n",
    "            # Perform backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Perform optimization\n",
    "            optimizer.step()\n",
    "\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if epoch % 5 == 0 or epoch == (epochs - 1):\n",
    "            test_loss, test_top1_acc, test_top5_acc = evaluate_model(model, testloader ,loss_function)\n",
    "            train_loss, train_top1_acc, train_top5_acc = evaluate_model(model, trainloader ,loss_function)\n",
    "            print(f'Train Acc: {train_top1_acc}')\n",
    "            print(f'Test Acc: {test_top1_acc}')\n",
    "        else:\n",
    "            test_loss, test_top1_acc, test_top5_acc = pd.NA, pd.NA, pd.NA\n",
    "            train_loss, train_top1_acc, train_top5_acc = pd.NA, pd.NA, pd.NA\n",
    "        \n",
    "        elapsed_time = round(end_time - st_time, 1)\n",
    "        train_entry = {'type': 'train', 'epoch': epoch, 'top1': train_top1_acc, 'top5': train_top5_acc,\n",
    "                       'loss': current_loss, 'time': elapsed_time}\n",
    "        \n",
    "        print(f'Loss: {current_loss}')\n",
    "        print(f'Time: {elapsed_time}')\n",
    "\n",
    "        test_entry = {'type': 'test', 'epoch': epoch, 'top1': test_top1_acc, 'top5': test_top5_acc,\n",
    "                      'loss': test_loss, 'time': pd.NA}\n",
    "        \n",
    "        entries.extend([train_entry, test_entry])\n",
    "        \n",
    "#         break\n",
    "\n",
    "\n",
    "    # Return trained model\n",
    "    return model, pd.DataFrame(entries), current_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fb17a306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:51:37.543755Z",
     "start_time": "2022-12-08T07:51:37.537749Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn = models.LayerwiseConfigurableCNN()\n",
    "# mlp = mlp.to(device)\n",
    "# cnn, cnn_df, loss = train_model(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2fdd5946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:51:37.697657Z",
     "start_time": "2022-12-08T07:51:37.689652Z"
    }
   },
   "outputs": [],
   "source": [
    "mlp = models.LayerwiseConfigurableMLP()\n",
    "# mlp = mlp.to(device)\n",
    "# mlp, mlp_df, loss = train_model(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "65ec36ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T08:29:16.972520Z",
     "start_time": "2022-12-08T08:29:16.966546Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_config_columns(results_df):\n",
    "    model_config = config.get_model_configuration()\n",
    "    results_df['optimizer'] = str(model_config['optimizer'])\n",
    "    results_df['hidden_layer_dim'] = model_config['hidden_layer_dim']\n",
    "    results_df['batch_size'] = model_config['batch_size']\n",
    "    results_df['invariant'] = global_config['invariant']\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fccf8e25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T16:13:44.591772Z",
     "start_time": "2022-12-08T16:13:44.577181Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def greedy_layerwise_training(model, rnd=0):\n",
    "    \"\"\" Perform greedy layer-wise training. \"\"\"    \n",
    "    print(\"NEW!\")\n",
    "    global_config = config.get_global_configuration()\n",
    "    device = global_config.get('device')\n",
    "    model = model.to(device)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Loss comparison\n",
    "    loss_comparable = float('inf')\n",
    "\n",
    "    # Iterate over the number of layers to add\n",
    "    training_losses = []\n",
    "    top5_accs = []\n",
    "    top1_accs = []\n",
    "    \n",
    "    dfs = []\n",
    "    for num_layers in range(global_config.get(\"num_layers_to_add\")):\n",
    "        if len(model.hidden_blocks) < num_layers:\n",
    "            # Add layer to model\n",
    "            model.add_hidden_block()\n",
    "            model = model.to(device)\n",
    "        \n",
    "        active_layer = self.input_block if num_layers == 0 else self.hidden_block[num_layers - 1]\n",
    "        model.activate_layers([active_layer])\n",
    "        if num_layers > 0:\n",
    "            model.freeze_layers([model.input_block] + [self.hidden_blocks[i] for i in range(num_layers-1)])\n",
    "        \n",
    "        # Print which model is trained\n",
    "        print(\"=\"*100)\n",
    "        if num_layers > 0:\n",
    "            print(f\">>> TRAINING THE MODEL WITH {num_layers} ADDITIONAL LAYERS:\")\n",
    "        else:\n",
    "            print(f\">>> TRAINING THE BASE MODEL:\")\n",
    "\n",
    "        # Train the model\n",
    "        model, df, end_loss = train_model(model, device=device, invariant=global_config['invariant'])\n",
    "        df['layer'] = num_layers\n",
    "        df['layer_params'] = model.num_trainable_weights()\n",
    "        dfs.append(df)\n",
    "\n",
    "        # Compare loss\n",
    "        if num_layers > 0 and end_loss < loss_comparable:\n",
    "            print(\"=\"*50)\n",
    "            print(f\">>> RESULTS: Adding this layer has improved the model loss from {loss_comparable} to {end_loss}\")\n",
    "            loss_comparable = end_loss\n",
    "        elif num_layers > 0:\n",
    "            print(\"=\"*50)\n",
    "            print(f\">>> RESULTS: Adding this layer did not improve the model loss from {loss_comparable} to {end_loss}\")\n",
    "        elif num_layers == 0:\n",
    "            loss_comparable = end_loss\n",
    "\n",
    "        # Add layer to model\n",
    "#         break\n",
    "\n",
    "    # Process is complete\n",
    "    print(\"Training process has finished.\")\n",
    "    \n",
    "    results_df = pd.concat(dfs)\n",
    "    results_df = add_config_columns(results_df)\n",
    "    results_df['model'] = model.get_name()\n",
    "\n",
    "    \n",
    "    return model, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7190a81b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T16:14:52.266850Z",
     "start_time": "2022-12-08T16:14:52.259933Z"
    }
   },
   "outputs": [],
   "source": [
    "def full_backprop_training(model):\n",
    "    \"\"\" Perform greedy layer-wise training. \"\"\"    \n",
    "    print(\"NEW!\")\n",
    "    global_config = config.get_global_configuration()\n",
    "    device = global_config.get('device')\n",
    "    model = model.to(device)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Loss comparison\n",
    "    loss_comparable = float('inf')\n",
    "\n",
    "    # Iterate over the number of layers to add\n",
    "    training_losses = []\n",
    "    top5_accs = []\n",
    "    top1_accs = []\n",
    "    \n",
    "    dfs = []\n",
    "    for i in range(global_config.get(\"num_layers_to_add\")):        \n",
    "        model = model.to(device)\n",
    "        model, df, end_loss = train_model(model, device=device, invariant=global_config['invariant'])\n",
    "        print(i)\n",
    "        print(end_loss)\n",
    "\n",
    "        df['layer'] = len(model.hidden_blocks)\n",
    "        df['layer_params'] = model.num_trainable_weights()\n",
    "        dfs.append(df)\n",
    "        \n",
    "        model.add_hidden_block()\n",
    "#         break\n",
    "    \n",
    "    results_df = pd.concat(dfs)\n",
    "    results_df = add_config_columns(results_df)\n",
    "    results_df['model'] = model.get_name()\n",
    "    \n",
    "    return model, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e075f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-08T07:04:53.888093Z",
     "start_time": "2022-12-08T07:04:53.885304Z"
    }
   },
   "source": [
    "# Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a134a7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-08T16:15:17.214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW!\n",
      "cpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Starting epoch 1\n",
      "Train Acc: 0.36598\n",
      "Test Acc: 0.3645\n",
      "Loss: 388.7958953380585\n",
      "Time: 13.6\n",
      "Starting epoch 2\n",
      "Loss: 358.3926159143448\n",
      "Time: 12.5\n",
      "Starting epoch 3\n",
      "Loss: 345.3890894651413\n",
      "Time: 12.4\n",
      "Starting epoch 4\n",
      "Loss: 336.8449845314026\n",
      "Time: 12.4\n",
      "Starting epoch 5\n",
      "Loss: 328.4441432952881\n",
      "Time: 13.3\n",
      "Starting epoch 6\n",
      "Train Acc: 0.44436\n",
      "Test Acc: 0.4379\n",
      "Loss: 322.1038612127304\n",
      "Time: 15.1\n",
      "Starting epoch 7\n",
      "Loss: 317.38067531585693\n",
      "Time: 16.0\n",
      "Starting epoch 8\n",
      "Loss: 312.77988970279694\n",
      "Time: 13.1\n",
      "Starting epoch 9\n",
      "Loss: 308.75481486320496\n",
      "Time: 15.0\n",
      "Starting epoch 10\n",
      "Loss: 304.6808851957321\n",
      "Time: 12.8\n",
      "Starting epoch 11\n",
      "Train Acc: 0.48186\n",
      "Test Acc: 0.4639\n",
      "Loss: 302.07377898693085\n",
      "Time: 13.3\n",
      "0\n",
      "302.07377898693085\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Starting epoch 1\n",
      "Train Acc: 0.45498\n",
      "Test Acc: 0.4414\n",
      "Loss: 319.44641733169556\n",
      "Time: 13.2\n",
      "Starting epoch 2\n",
      "Loss: 310.14966428279877\n",
      "Time: 15.7\n",
      "Starting epoch 3\n",
      "Loss: 305.1170837879181\n",
      "Time: 13.2\n",
      "Starting epoch 4\n",
      "Loss: 301.71063697338104\n",
      "Time: 12.6\n",
      "Starting epoch 5\n",
      "Loss: 297.95042300224304\n",
      "Time: 15.3\n",
      "Starting epoch 6\n",
      "Train Acc: 0.48848\n",
      "Test Acc: 0.4684\n",
      "Loss: 295.4971294403076\n",
      "Time: 12.9\n",
      "Starting epoch 7\n",
      "Loss: 292.31508123874664\n",
      "Time: 14.2\n",
      "Starting epoch 8\n",
      "Loss: 289.9228676557541\n",
      "Time: 13.9\n",
      "Starting epoch 9\n",
      "Loss: 288.08222806453705\n",
      "Time: 13.9\n",
      "Starting epoch 10\n",
      "Loss: 285.52730762958527\n",
      "Time: 13.3\n",
      "Starting epoch 11\n",
      "Train Acc: 0.51846\n",
      "Test Acc: 0.493\n",
      "Loss: 282.7734549045563\n",
      "Time: 13.2\n",
      "1\n",
      "282.7734549045563\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Starting epoch 1\n",
      "Train Acc: 0.51326\n",
      "Test Acc: 0.4898\n",
      "Loss: 287.04749155044556\n",
      "Time: 13.7\n",
      "Starting epoch 2\n",
      "Loss: 283.69262886047363\n",
      "Time: 12.6\n",
      "Starting epoch 3\n",
      "Loss: 281.883332490921\n",
      "Time: 13.0\n",
      "Starting epoch 4\n",
      "Loss: 279.91773521900177\n",
      "Time: 12.8\n",
      "Starting epoch 5\n",
      "Loss: 278.4934003353119\n",
      "Time: 12.4\n",
      "Starting epoch 6\n",
      "Train Acc: 0.52618\n",
      "Test Acc: 0.4973\n",
      "Loss: 276.7004737854004\n",
      "Time: 13.3\n",
      "Starting epoch 7\n",
      "Loss: 275.180752158165\n",
      "Time: 13.0\n",
      "Starting epoch 8\n",
      "Loss: 273.3036252260208\n",
      "Time: 13.7\n",
      "Starting epoch 9\n",
      "Loss: 272.3530012369156\n",
      "Time: 15.2\n",
      "Starting epoch 10\n",
      "Loss: 270.73006904125214\n",
      "Time: 14.7\n",
      "Starting epoch 11\n",
      "Train Acc: 0.54022\n",
      "Test Acc: 0.5064\n",
      "Loss: 269.26107025146484\n",
      "Time: 14.1\n",
      "2\n",
      "269.26107025146484\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Starting epoch 1\n",
      "Train Acc: 0.53914\n",
      "Test Acc: 0.5044\n",
      "Loss: 268.47519993782043\n",
      "Time: 13.3\n",
      "Starting epoch 2\n",
      "Loss: 266.85744202136993\n",
      "Time: 12.9\n",
      "Starting epoch 3\n",
      "Loss: 265.3191616535187\n",
      "Time: 12.8\n",
      "Starting epoch 4\n",
      "Loss: 264.4316725730896\n",
      "Time: 12.6\n",
      "Starting epoch 5\n",
      "Loss: 263.26087284088135\n",
      "Time: 12.9\n",
      "Starting epoch 6\n",
      "Train Acc: 0.5501\n",
      "Test Acc: 0.5088\n",
      "Loss: 262.13556265830994\n",
      "Time: 13.5\n",
      "Starting epoch 7\n",
      "Loss: 260.78305888175964\n",
      "Time: 12.7\n",
      "Starting epoch 8\n",
      "Loss: 259.6511263847351\n",
      "Time: 12.6\n",
      "Starting epoch 9\n",
      "Loss: 258.29595267772675\n",
      "Time: 15.0\n",
      "Starting epoch 10\n",
      "Loss: 257.24918282032013\n",
      "Time: 13.2\n",
      "Starting epoch 11\n",
      "Train Acc: 0.5602\n",
      "Test Acc: 0.5173\n",
      "Loss: 255.7751488685608\n",
      "Time: 13.2\n",
      "3\n",
      "255.7751488685608\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Starting epoch 1\n",
      "Train Acc: 0.56632\n",
      "Test Acc: 0.5196\n",
      "Loss: 255.62786519527435\n",
      "Time: 13.9\n",
      "Starting epoch 2\n",
      "Loss: 253.92109894752502\n",
      "Time: 14.3\n",
      "Starting epoch 3\n",
      "Loss: 253.24731922149658\n",
      "Time: 12.7\n",
      "Starting epoch 4\n",
      "Loss: 252.58513462543488\n",
      "Time: 13.3\n",
      "Starting epoch 5\n",
      "Loss: 251.01572906970978\n",
      "Time: 12.9\n",
      "Starting epoch 6\n",
      "Train Acc: 0.57374\n",
      "Test Acc: 0.5174\n",
      "Loss: 249.9542167186737\n",
      "Time: 13.3\n",
      "Starting epoch 7\n"
     ]
    }
   ],
   "source": [
    "mlp_bp_model, mlp_bp_results_df = full_backprop_training(models.LayerwiseConfigurableMLP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1aa4a9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-08T16:15:18.314Z"
    }
   },
   "outputs": [],
   "source": [
    "mlp_model, mlp_results_df = greedy_layerwise_training(models.LayerwiseConfigurableMLP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cd1a4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-08T16:15:19.306Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_model, cnn_results_df = greedy_layerwise_training(models.LayerwiseConfigurableCNN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532fb1cd",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-12-08T16:15:19.798Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn_bp_model, cnn_bp_results_df = full_backprop_training(models.LayerwiseConfigurableCNN())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d357978",
   "metadata": {},
   "source": [
    "Questions\n",
    "\n",
    "1) Understanding the curvature of the loss function - how to compute the Hessian\n",
    "2) Should I freeze the output layer?\n",
    "3) What does it mean to set W_L using the neural collapse property?\n",
    "4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ce5cd",
   "metadata": {},
   "source": [
    "Lecture\n",
    "\n",
    "1) Understanding, as a mathematician, the critical points of L\n",
    "2) Goal\n",
    "\n",
    "\n",
    "For deep (l >= 3) nonlinear networks, bad local (non-global) minimima exist - that are difficult to escape\n",
    "Morse Function. A function L: R^d -> R is MOrse if at every critical point p in R^d the Hessian Hess(L) (p) is nonsingular (i.e. has no 0 eigenvalues)\n",
    "\n",
    "1) If L is Morse, can understand the topology of u by computing all the critical points of L and geometry near them\n",
    "2) Almost every c^2 function is Morse (Morse functions are open, dense in C^2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "324px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
